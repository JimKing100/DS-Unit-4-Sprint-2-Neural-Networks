{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled63.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP1a30aOUSTDGJNS5WvDP2U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JimKing100/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/Sprint_Prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxWa-ocWOmh9",
        "colab_type": "text"
      },
      "source": [
        "### How a Neural Network and It's Components Work\n",
        "\n",
        "ANN's are computational models inspired by neural networks in the brain.  ANN's are composed of three types of layers = input, hidden and output layers.  The input layer receives inputs from the dataset and is exposed to the dataset.  The hidden layer in not exposed directly to the data and consists of weights and calculations used to analyze the data.  The output layer represents the results or output of the model.  Each layer is composed of nodes.  The complete architecture of an ANN can be visualized in a Node Map.\n",
        "\n",
        "The links between nodes in different layers represent weights.  In a feed-forward NN, each layer affects the next layer by a weighted sum of inputs plus a bias factor.  The optimal weights and biases of a NN can be searched through gradient descent if there is a loss function evaluating the quality of the predictions compared to the y values of the training data.\n",
        "\n",
        "In NN each node has an activation function.  An activation function decides how much signal to pass to the next layer.\n",
        "\n",
        "A perceptron in a simple neural network that takes input values, multiplies by weights, adds a bias,sums the products, passes the sum through an activation function and outputs a final value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbkeTBSARm7Y",
        "colab_type": "text"
      },
      "source": [
        "### Basics of Backpropagation\n",
        "\n",
        "In order to evaluate a NN's performance, data is \"fed forward\" until predictions are obtained and then the \"loss\" or \"error\" for a given observation is ascertained by looking at what the network predicted for that observation and comparing it to what it should have predicted.\n",
        "\n",
        "The error for a given observation is calculated by taking the square of the difference between the predicted value and the actual value.  The overall quality of a network's predictions can be found by finding the average error across all observations. This gives us the \"Mean Squared Error.\"\n",
        "\n",
        "An \"epoch\" is one cycle of passing our data forward through the network, measuring the error given our specified cost function, and then, via gradient descent,updating weights within our network to hopefully improve the quality of our predictions on the next iteration.\n",
        "\n",
        "Backpropagation refers to a specific algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch.\n",
        "\n",
        "4 steps for backpropagation:\n",
        "\n",
        "1) Calculate Error for a given each observation\n",
        "\n",
        "2) Does the error indicate that I'm overestimating or underestimating in my prediction?\n",
        "\n",
        "3) Look at final layer weights to get an idea for which weights are helping pass desireable signals and which are stifling desireable signals.\n",
        "\n",
        "4) Also go to the previous layer and see what can be done to boost activations that are associated with helpful weights, and limit activations that are associated with unhelpful weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUwW0HCiV07u",
        "colab_type": "text"
      },
      "source": [
        "### Build and Train a Perceptron Using Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXNge0GnOOZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAP0NFRHQwFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I want activations that correspond to negative weights to be lower\n",
        "# and activations that correspond to positive weights to be higher\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        # Set up Architecture of Neural Network\n",
        "        self.inputs = 3\n",
        "        self.hiddenNodes = 4\n",
        "        self.outputNodes = 1\n",
        "\n",
        "        # Initial Weights\n",
        "        # 3x4 Matrix Array for the First Layer\n",
        "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
        "       \n",
        "        # 4x1 Matrix Array for Hidden to Output\n",
        "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1+np.exp(-s))\n",
        "    \n",
        "    def sigmoidPrime(self, s):\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def feed_forward(self, X):\n",
        "        \"\"\"\n",
        "        Calculate the NN inference using feed forward.\n",
        "        aka \"predict\"\n",
        "        \"\"\"\n",
        "        \n",
        "        # Weighted sum of inputs => hidden layer\n",
        "        self.hidden_sum = np.dot(X, self.weights1)\n",
        "        \n",
        "        # Activations of weighted sum\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "        \n",
        "        # Weight sum between hidden and output\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
        "        \n",
        "        # Final activation of output\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "        \n",
        "        return self.activated_output\n",
        "        \n",
        "    def backward(self, X,y,o):\n",
        "        \"\"\"\n",
        "        Backward propagate through the network\n",
        "        \"\"\"\n",
        "        \n",
        "        # Error in Output\n",
        "        self.o_error = y - o\n",
        "        \n",
        "        # Apply Derivative of Sigmoid to error\n",
        "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
        "        # ^- aka hidden => output\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
        "        \n",
        "        # z2 error\n",
        "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
        "        \n",
        "        # How much of that \"far off\" can explained by the input => hidden\n",
        "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
        "        \n",
        "        # Adjustment to first set of weights (input => hidden)\n",
        "        self.weights1 += X.T.dot(self.z2_delta)\n",
        "        # Adjustment to second set of weights (hidden => output)\n",
        "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
        "        \n",
        "\n",
        "    def train(self, X, y):\n",
        "        o = self.feed_forward(X)\n",
        "        self.backward(X,y,o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yVX8U6FQ7kD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array([\n",
        "              [0, 0, 1],\n",
        "              [0, 1, 1],\n",
        "              [1, 0, 1],\n",
        "              [0, 1, 0],\n",
        "              [1, 0, 0],\n",
        "              [1, 1, 1],\n",
        "              [0, 0, 0]\n",
        "\n",
        "])\n",
        "\n",
        "y = np.array([\n",
        "              [0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0],\n",
        "              [0]\n",
        "\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6hSglYpRQE_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "926194fa-2194-4c12-d1da-3e8a0b59e30b"
      },
      "source": [
        "nn = NeuralNetwork()\n",
        "\n",
        "# Number of Epochs / Iterations\n",
        "for i in range(10000):\n",
        "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
        "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
        "        print('Input: \\n', X)\n",
        "        print('Actual Output: \\n', y)\n",
        "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
        "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
        "    nn.train(X,y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------EPOCH 1---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.76172105]\n",
            " [0.80179543]\n",
            " [0.78105107]\n",
            " [0.77092938]\n",
            " [0.74170987]\n",
            " [0.81581361]\n",
            " [0.7184885 ]]\n",
            "Loss: \n",
            " 0.2812010514425749\n",
            "+---------EPOCH 2---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.6817056 ]\n",
            " [0.71363969]\n",
            " [0.69914086]\n",
            " [0.69026655]\n",
            " [0.67113871]\n",
            " [0.72752208]\n",
            " [0.65172613]]\n",
            "Loss: \n",
            " 0.25648012229283385\n",
            "+---------EPOCH 3---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.61714745]\n",
            " [0.63850265]\n",
            " [0.63166578]\n",
            " [0.62399969]\n",
            " [0.61516414]\n",
            " [0.65076503]\n",
            " [0.59964887]]\n",
            "Loss: \n",
            " 0.24568145243752057\n",
            "+---------EPOCH 4---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.5855495 ]\n",
            " [0.60095867]\n",
            " [0.59844925]\n",
            " [0.59147096]\n",
            " [0.58813806]\n",
            " [0.61213583]\n",
            " [0.57458003]]\n",
            "Loss: \n",
            " 0.2435319895903469\n",
            "+---------EPOCH 5---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.57477361]\n",
            " [0.58823647]\n",
            " [0.58731271]\n",
            " [0.58066315]\n",
            " [0.57931703]\n",
            " [0.59917401]\n",
            " [0.5662184 ]]\n",
            "Loss: \n",
            " 0.2432364338382637\n",
            "+---------EPOCH 1000---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.0072045 ]\n",
            " [0.9501337 ]\n",
            " [0.95786347]\n",
            " [0.95210077]\n",
            " [0.95173937]\n",
            " [0.0544335 ]\n",
            " [0.08603636]]\n",
            "Loss: \n",
            " 0.002757532233059195\n",
            "+---------EPOCH 2000---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.00392863]\n",
            " [0.96843912]\n",
            " [0.97262518]\n",
            " [0.9683565 ]\n",
            " [0.96889173]\n",
            " [0.03149567]\n",
            " [0.05880662]]\n",
            "Loss: \n",
            " 0.0011685905918035109\n",
            "+---------EPOCH 3000---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.00287771]\n",
            " [0.97516   ]\n",
            " [0.97816091]\n",
            " [0.97459754]\n",
            " [0.97527576]\n",
            " [0.02385727]\n",
            " [0.04766969]]\n",
            "Loss: \n",
            " 0.0007429134791597113\n",
            "+---------EPOCH 4000---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.00232905]\n",
            " [0.97890918]\n",
            " [0.9812848 ]\n",
            " [0.97814644]\n",
            " [0.97885574]\n",
            " [0.01982718]\n",
            " [0.04117275]]\n",
            "Loss: \n",
            " 0.0005447822469270061\n",
            "+---------EPOCH 5000---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.00198264]\n",
            " [0.98138034]\n",
            " [0.98335901]\n",
            " [0.98051326]\n",
            " [0.98122219]\n",
            " [0.01727222]\n",
            " [0.03677614]]\n",
            "Loss: \n",
            " 0.0004300998470391598\n",
            "+---------EPOCH 6000---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.00174013]\n",
            " [0.98316555]\n",
            " [0.98486516]\n",
            " [0.98223737]\n",
            " [0.98293491]\n",
            " [0.0154805 ]\n",
            " [0.03354242]]\n",
            "Loss: \n",
            " 0.00035527971786941377\n",
            "+---------EPOCH 7000---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.00155893]\n",
            " [0.9845327 ]\n",
            " [0.98602295]\n",
            " [0.98356618]\n",
            " [0.98424824]\n",
            " [0.01414078]\n",
            " [0.03103286]]\n",
            "Loss: \n",
            " 0.0003026019340163364\n",
            "+---------EPOCH 8000---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.0014173 ]\n",
            " [0.98562294]\n",
            " [0.98694891]\n",
            " [0.98463132]\n",
            " [0.98529658]\n",
            " [0.0130935 ]\n",
            " [0.02901074]]\n",
            "Loss: \n",
            " 0.00026349842158108767\n",
            "+---------EPOCH 9000---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.00130291]\n",
            " [0.98651862]\n",
            " [0.98771136]\n",
            " [0.98551016]\n",
            " [0.9861585 ]\n",
            " [0.01224764]\n",
            " [0.02733544]]\n",
            "Loss: \n",
            " 0.00023331848876750517\n",
            "+---------EPOCH 10000---------+\n",
            "Input: \n",
            " [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n",
            "Actual Output: \n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Predicted Output: \n",
            " [[0.00120816]\n",
            " [0.98727144]\n",
            " [0.98835336]\n",
            " [0.9862516 ]\n",
            " [0.98688339]\n",
            " [0.01154717]\n",
            " [0.0259174 ]]\n",
            "Loss: \n",
            " 0.0002093190018205515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbynLmyuV-w1",
        "colab_type": "text"
      },
      "source": [
        "### Build, Train and Hyperparameter Tune a MLP with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvJ7eYDIWJ9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See Assignment"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}