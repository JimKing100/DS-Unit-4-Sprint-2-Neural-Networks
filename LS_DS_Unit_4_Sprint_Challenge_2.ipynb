{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:**\n",
    "\n",
    "ANN's are computational models inspired by neural networks in the brain.  The neurons or \"nodes\" receive inputs and pass on their signal to the next layer of nodes if a certain threshold is reached.\n",
    "\n",
    "- **Input Layer:**\n",
    "\n",
    "ANN's are composed of three types of layers - input, hidden and output layers. The input layer receives inputs from the dataset and is exposed to the dataset. \n",
    "\n",
    "- **Hidden Layer:**\n",
    "\n",
    "The hidden layer in not exposed directly to the data and consists of weights and calculations used to analyze the data.\n",
    "\n",
    "- **Output Layer:**\n",
    "\n",
    "The output layer represents the results or output of the model. \n",
    "\n",
    "- **Activation:**\n",
    "\n",
    "In a NN each node has an activation function. An activation function decides how much signal to pass to the next layer.\n",
    "\n",
    "- **Backpropagation:**\n",
    "\n",
    "In order to evaluate a NN's performance, data is \"fed forward\" until predictions are obtained and then the \"loss\" or \"error\" for a given observation is ascertained by looking at what the network predicted for that observation and comparing it to what it should have predicted.\n",
    "\n",
    "The error for a given observation is calculated by taking the square of the difference between the predicted value and the actual value. The overall quality of a network's predictions can be found by finding the average error across all observations. This gives us the \"Mean Squared Error.\"\n",
    "\n",
    "An \"epoch\" is one cycle of passing our data forward through the network, measuring the error given our specified cost function, and then, via gradient descent, updating weights within our network to hopefully improve the quality of our predictions on the next iteration.\n",
    "\n",
    "Backpropagation refers to a specific algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch.\n",
    "\n",
    "4 steps for backpropagation:\n",
    "\n",
    "1 - Calculate Error for a given each observation\n",
    "\n",
    "2 - Does the error indicate that I'm overestimating or underestimating in my prediction?\n",
    "\n",
    "3 - Look at final layer weights to get an idea for which weights are helping pass desireable signals and which are stifling desireable signals.\n",
    "\n",
    "4 - Also go to the previous layer and see what can be done to boost activations that are associated with helpful weights, and limit activations that are associated with unhelpful weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('https://raw.githubusercontent.com/JimKing100/DS-Unit-4-Sprint-2-Neural-Networks/master/chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Start your candy perceptron here\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function np.exp(-x) is e**-x\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivate(x):\n",
    "  sx = sigmoid(x)\n",
    "  return sx * (1-sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, niter = 10000):\n",
    "        self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        sx = sigmoid(x)\n",
    "        return sx * (1-sx)\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # Randomly Initialize Weights\n",
    "        self.weight = np.zeros(X.shape[1])\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            # Weighted sum of inputs / weights\n",
    "            weighted_sum = np.dot(X, self.weight)\n",
    "\n",
    "            # Activate!\n",
    "            activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "            # Calc error\n",
    "            error = y - activated_output\n",
    "            adjustments = error * sigmoid_derivate(activated_output)\n",
    "\n",
    "            # Update the Weights\n",
    "            self.weight += np.dot(X.T, adjustments)\n",
    "\n",
    "        return activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JKMacBook/opt/anaconda3/envs/NN/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn = Perceptron()\n",
    "y_pred = pn.predict(X, y)\n",
    "test = y_pred.astype(int)\n",
    "test\n",
    "accuracy_score(y, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple perceptron uses only forward-feed and does not use the loss to improve the accuracy with backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# I want activations that correspond to negative weights to be lower\n",
    "# and activations that correspond to positive weights to be higher\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 1\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x1 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 1x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "        \n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # How much of that \"far off\" can explained by the input => hidden\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        # Adjustment to first set of weights (input => hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        # Adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.55703286]\n",
      " [0.57084951]\n",
      " [0.55703286]\n",
      " ...\n",
      " [0.55703286]\n",
      " [0.55703286]\n",
      " [0.57084951]]\n",
      "Loss: \n",
      " 0.25401782264143047\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.01007405]\n",
      " [0.44754282]\n",
      " [0.01007405]\n",
      " ...\n",
      " [0.01007405]\n",
      " [0.01007405]\n",
      " [0.44754282]]\n",
      "Loss: \n",
      " 0.3819209627733153\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.20115\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.20115\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.20115\n",
      "+---------EPOCH 1000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.20115\n",
      "+---------EPOCH 2000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.20115\n",
      "+---------EPOCH 3000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.20115\n",
      "+---------EPOCH 4000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.20115\n",
      "+---------EPOCH 5000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.20115\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(5000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP uses backprogagation to improve the performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "151   71    0   0       112   149    0        1      125      0      1.6   \n",
       "55    52    1   1       134   201    0        1      158      0      0.8   \n",
       "219   48    1   0       130   256    1        0      150      1      0.0   \n",
       "264   54    1   0       110   206    0        0      108      1      0.0   \n",
       "56    48    1   0       122   222    0        0      186      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "151      1   0     2       1  \n",
       "55       2   1     2       1  \n",
       "219      2   2     3       0  \n",
       "264      1   1     2       0  \n",
       "56       2   0     2       1  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    " \n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer to array\n",
    "train = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(272, 14) (31, 14)\n"
     ]
    }
   ],
   "source": [
    "# Train, test split\n",
    "train, test = train_test_split(train, train_size=0.90, test_size=0.10, random_state=42)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(train[:,0:13])\n",
    "Y = train[:,13]\n",
    "X_test = scaler.fit_transform(test[:,0:13])\n",
    "Y_test = test[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=13, activation='relu'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "          \n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = 'adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 272 samples, validate on 31 samples\n",
      "Epoch 1/100\n",
      "272/272 [==============================] - 2s 6ms/sample - loss: 0.7061 - acc: 0.5257 - val_loss: 0.5575 - val_acc: 0.7419\n",
      "Epoch 2/100\n",
      "272/272 [==============================] - 0s 470us/sample - loss: 0.6470 - acc: 0.5993 - val_loss: 0.5246 - val_acc: 0.7742\n",
      "Epoch 3/100\n",
      "272/272 [==============================] - 0s 418us/sample - loss: 0.6272 - acc: 0.6434 - val_loss: 0.4994 - val_acc: 0.7742\n",
      "Epoch 4/100\n",
      "272/272 [==============================] - 0s 383us/sample - loss: 0.5668 - acc: 0.7353 - val_loss: 0.4700 - val_acc: 0.9032\n",
      "Epoch 5/100\n",
      "272/272 [==============================] - 0s 454us/sample - loss: 0.6093 - acc: 0.7243 - val_loss: 0.4457 - val_acc: 0.9032\n",
      "Epoch 6/100\n",
      "272/272 [==============================] - 0s 443us/sample - loss: 0.5278 - acc: 0.7426 - val_loss: 0.4219 - val_acc: 0.9355\n",
      "Epoch 7/100\n",
      "272/272 [==============================] - 0s 435us/sample - loss: 0.5487 - acc: 0.7721 - val_loss: 0.4042 - val_acc: 0.9355\n",
      "Epoch 8/100\n",
      "272/272 [==============================] - 0s 602us/sample - loss: 0.5127 - acc: 0.7610 - val_loss: 0.3947 - val_acc: 0.9355\n",
      "Epoch 9/100\n",
      "272/272 [==============================] - 0s 414us/sample - loss: 0.4992 - acc: 0.7831 - val_loss: 0.3787 - val_acc: 0.9355\n",
      "Epoch 10/100\n",
      "272/272 [==============================] - 0s 461us/sample - loss: 0.4778 - acc: 0.7721 - val_loss: 0.3699 - val_acc: 0.9355\n",
      "Epoch 11/100\n",
      "272/272 [==============================] - 0s 436us/sample - loss: 0.4347 - acc: 0.8235 - val_loss: 0.3561 - val_acc: 0.9355\n",
      "Epoch 12/100\n",
      "272/272 [==============================] - 0s 563us/sample - loss: 0.4871 - acc: 0.7647 - val_loss: 0.3498 - val_acc: 0.9032\n",
      "Epoch 13/100\n",
      "272/272 [==============================] - 0s 388us/sample - loss: 0.4269 - acc: 0.8088 - val_loss: 0.3437 - val_acc: 0.9355\n",
      "Epoch 14/100\n",
      "272/272 [==============================] - 0s 462us/sample - loss: 0.4224 - acc: 0.8346 - val_loss: 0.3356 - val_acc: 0.9032\n",
      "Epoch 15/100\n",
      "272/272 [==============================] - 0s 428us/sample - loss: 0.4458 - acc: 0.7978 - val_loss: 0.3322 - val_acc: 0.9032\n",
      "Epoch 16/100\n",
      "272/272 [==============================] - 0s 937us/sample - loss: 0.4266 - acc: 0.8015 - val_loss: 0.3304 - val_acc: 0.9032\n",
      "Epoch 17/100\n",
      "272/272 [==============================] - 0s 935us/sample - loss: 0.3909 - acc: 0.8419 - val_loss: 0.3298 - val_acc: 0.9032\n",
      "Epoch 18/100\n",
      "272/272 [==============================] - 0s 395us/sample - loss: 0.3866 - acc: 0.8235 - val_loss: 0.3310 - val_acc: 0.8710\n",
      "Epoch 19/100\n",
      "272/272 [==============================] - 0s 435us/sample - loss: 0.4268 - acc: 0.8199 - val_loss: 0.3306 - val_acc: 0.8710\n",
      "Epoch 20/100\n",
      "272/272 [==============================] - 0s 478us/sample - loss: 0.3980 - acc: 0.8125 - val_loss: 0.3296 - val_acc: 0.9032\n",
      "Epoch 21/100\n",
      "272/272 [==============================] - 0s 745us/sample - loss: 0.4184 - acc: 0.8382 - val_loss: 0.3242 - val_acc: 0.9032\n",
      "Epoch 22/100\n",
      "272/272 [==============================] - 0s 692us/sample - loss: 0.3969 - acc: 0.8162 - val_loss: 0.3229 - val_acc: 0.9032\n",
      "Epoch 23/100\n",
      "272/272 [==============================] - 0s 457us/sample - loss: 0.3909 - acc: 0.8199 - val_loss: 0.3288 - val_acc: 0.9032\n",
      "Epoch 24/100\n",
      "272/272 [==============================] - 0s 555us/sample - loss: 0.3791 - acc: 0.8199 - val_loss: 0.3280 - val_acc: 0.9032\n",
      "Epoch 25/100\n",
      "272/272 [==============================] - 0s 504us/sample - loss: 0.3826 - acc: 0.8346 - val_loss: 0.3287 - val_acc: 0.9032\n",
      "Epoch 26/100\n",
      "272/272 [==============================] - 0s 800us/sample - loss: 0.3836 - acc: 0.8419 - val_loss: 0.3276 - val_acc: 0.9032\n",
      "Epoch 27/100\n",
      "272/272 [==============================] - 0s 1ms/sample - loss: 0.3833 - acc: 0.8419 - val_loss: 0.3273 - val_acc: 0.9032\n",
      "Epoch 28/100\n",
      "272/272 [==============================] - 0s 955us/sample - loss: 0.3749 - acc: 0.8272 - val_loss: 0.3296 - val_acc: 0.9032\n",
      "Epoch 29/100\n",
      "272/272 [==============================] - 0s 578us/sample - loss: 0.3660 - acc: 0.8382 - val_loss: 0.3306 - val_acc: 0.9032\n",
      "Epoch 30/100\n",
      "272/272 [==============================] - 0s 467us/sample - loss: 0.3825 - acc: 0.8346 - val_loss: 0.3278 - val_acc: 0.9032\n",
      "Epoch 31/100\n",
      "272/272 [==============================] - 0s 786us/sample - loss: 0.3645 - acc: 0.8603 - val_loss: 0.3288 - val_acc: 0.9032\n",
      "Epoch 32/100\n",
      "272/272 [==============================] - 0s 1ms/sample - loss: 0.3480 - acc: 0.8309 - val_loss: 0.3249 - val_acc: 0.9032\n",
      "Epoch 33/100\n",
      "272/272 [==============================] - 0s 575us/sample - loss: 0.3866 - acc: 0.8272 - val_loss: 0.3275 - val_acc: 0.9032\n",
      "Epoch 34/100\n",
      "272/272 [==============================] - 0s 398us/sample - loss: 0.3786 - acc: 0.8603 - val_loss: 0.3319 - val_acc: 0.9032\n",
      "Epoch 35/100\n",
      "272/272 [==============================] - 0s 397us/sample - loss: 0.3645 - acc: 0.8309 - val_loss: 0.3391 - val_acc: 0.9032\n",
      "Epoch 36/100\n",
      "272/272 [==============================] - 0s 418us/sample - loss: 0.3405 - acc: 0.8713 - val_loss: 0.3401 - val_acc: 0.9032\n",
      "Epoch 37/100\n",
      "272/272 [==============================] - 0s 444us/sample - loss: 0.3702 - acc: 0.8419 - val_loss: 0.3442 - val_acc: 0.9032\n",
      "Epoch 38/100\n",
      "272/272 [==============================] - 0s 1ms/sample - loss: 0.3845 - acc: 0.8346 - val_loss: 0.3429 - val_acc: 0.9032\n",
      "Epoch 39/100\n",
      "272/272 [==============================] - 0s 2ms/sample - loss: 0.3423 - acc: 0.8456 - val_loss: 0.3451 - val_acc: 0.9032\n",
      "Epoch 40/100\n",
      "272/272 [==============================] - 0s 756us/sample - loss: 0.3705 - acc: 0.8640 - val_loss: 0.3461 - val_acc: 0.9032\n",
      "Epoch 41/100\n",
      "272/272 [==============================] - 0s 828us/sample - loss: 0.3805 - acc: 0.8419 - val_loss: 0.3488 - val_acc: 0.9032\n",
      "Epoch 42/100\n",
      "272/272 [==============================] - 0s 580us/sample - loss: 0.3299 - acc: 0.8603 - val_loss: 0.3517 - val_acc: 0.9032\n",
      "Epoch 43/100\n",
      "272/272 [==============================] - 0s 551us/sample - loss: 0.3584 - acc: 0.8456 - val_loss: 0.3515 - val_acc: 0.9032\n",
      "Epoch 44/100\n",
      "272/272 [==============================] - 0s 390us/sample - loss: 0.3601 - acc: 0.8346 - val_loss: 0.3517 - val_acc: 0.9032\n",
      "Epoch 45/100\n",
      "272/272 [==============================] - 0s 431us/sample - loss: 0.3898 - acc: 0.8382 - val_loss: 0.3579 - val_acc: 0.9032\n",
      "Epoch 46/100\n",
      "272/272 [==============================] - 0s 525us/sample - loss: 0.3473 - acc: 0.8456 - val_loss: 0.3544 - val_acc: 0.9032\n",
      "Epoch 47/100\n",
      "272/272 [==============================] - 0s 537us/sample - loss: 0.3402 - acc: 0.8676 - val_loss: 0.3540 - val_acc: 0.9032\n",
      "Epoch 48/100\n",
      "272/272 [==============================] - 0s 608us/sample - loss: 0.3525 - acc: 0.8566 - val_loss: 0.3563 - val_acc: 0.9032\n",
      "Epoch 49/100\n",
      "272/272 [==============================] - 0s 590us/sample - loss: 0.3469 - acc: 0.8456 - val_loss: 0.3571 - val_acc: 0.9032\n",
      "Epoch 50/100\n",
      "272/272 [==============================] - 0s 554us/sample - loss: 0.3135 - acc: 0.8750 - val_loss: 0.3611 - val_acc: 0.9032\n",
      "Epoch 51/100\n",
      "272/272 [==============================] - 0s 416us/sample - loss: 0.3582 - acc: 0.8309 - val_loss: 0.3624 - val_acc: 0.9032\n",
      "Epoch 52/100\n",
      "272/272 [==============================] - 0s 541us/sample - loss: 0.3446 - acc: 0.8419 - val_loss: 0.3659 - val_acc: 0.9032\n",
      "Epoch 53/100\n",
      "272/272 [==============================] - 0s 559us/sample - loss: 0.3249 - acc: 0.8676 - val_loss: 0.3665 - val_acc: 0.9032\n",
      "Epoch 54/100\n",
      "272/272 [==============================] - 0s 431us/sample - loss: 0.3364 - acc: 0.8456 - val_loss: 0.3680 - val_acc: 0.9032\n",
      "Epoch 55/100\n",
      "272/272 [==============================] - 0s 422us/sample - loss: 0.3208 - acc: 0.8566 - val_loss: 0.3695 - val_acc: 0.9032\n",
      "Epoch 56/100\n",
      "272/272 [==============================] - 0s 557us/sample - loss: 0.3333 - acc: 0.8493 - val_loss: 0.3738 - val_acc: 0.9032\n",
      "Epoch 57/100\n",
      "272/272 [==============================] - 0s 565us/sample - loss: 0.3255 - acc: 0.8566 - val_loss: 0.3734 - val_acc: 0.9032\n",
      "Epoch 58/100\n",
      "272/272 [==============================] - 0s 507us/sample - loss: 0.3444 - acc: 0.8676 - val_loss: 0.3737 - val_acc: 0.9032\n",
      "Epoch 59/100\n",
      "272/272 [==============================] - 0s 422us/sample - loss: 0.3196 - acc: 0.8566 - val_loss: 0.3760 - val_acc: 0.9032\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 407us/sample - loss: 0.3352 - acc: 0.8456 - val_loss: 0.3834 - val_acc: 0.9032\n",
      "Epoch 61/100\n",
      "272/272 [==============================] - 0s 384us/sample - loss: 0.3261 - acc: 0.8529 - val_loss: 0.3852 - val_acc: 0.9032\n",
      "Epoch 62/100\n",
      "272/272 [==============================] - 0s 374us/sample - loss: 0.3352 - acc: 0.8566 - val_loss: 0.3834 - val_acc: 0.9032\n",
      "Epoch 63/100\n",
      "272/272 [==============================] - 0s 759us/sample - loss: 0.3249 - acc: 0.8713 - val_loss: 0.3867 - val_acc: 0.9032\n",
      "Epoch 64/100\n",
      "272/272 [==============================] - 0s 516us/sample - loss: 0.3112 - acc: 0.8603 - val_loss: 0.3842 - val_acc: 0.9032\n",
      "Epoch 65/100\n",
      "272/272 [==============================] - 0s 549us/sample - loss: 0.3231 - acc: 0.8750 - val_loss: 0.3875 - val_acc: 0.9032\n",
      "Epoch 66/100\n",
      "272/272 [==============================] - 0s 448us/sample - loss: 0.3318 - acc: 0.8676 - val_loss: 0.3870 - val_acc: 0.9032\n",
      "Epoch 67/100\n",
      "272/272 [==============================] - 0s 440us/sample - loss: 0.3016 - acc: 0.8860 - val_loss: 0.3832 - val_acc: 0.9032\n",
      "Epoch 68/100\n",
      "272/272 [==============================] - 0s 462us/sample - loss: 0.3265 - acc: 0.8529 - val_loss: 0.3906 - val_acc: 0.9032\n",
      "Epoch 69/100\n",
      "272/272 [==============================] - 0s 669us/sample - loss: 0.3185 - acc: 0.8934 - val_loss: 0.3937 - val_acc: 0.9032\n",
      "Epoch 70/100\n",
      "272/272 [==============================] - 0s 608us/sample - loss: 0.2835 - acc: 0.8787 - val_loss: 0.4017 - val_acc: 0.9032\n",
      "Epoch 71/100\n",
      "272/272 [==============================] - 0s 1ms/sample - loss: 0.3118 - acc: 0.8860 - val_loss: 0.4048 - val_acc: 0.9032\n",
      "Epoch 72/100\n",
      "272/272 [==============================] - 0s 854us/sample - loss: 0.3230 - acc: 0.8603 - val_loss: 0.4058 - val_acc: 0.9032\n",
      "Epoch 73/100\n",
      "272/272 [==============================] - 0s 858us/sample - loss: 0.3327 - acc: 0.8787 - val_loss: 0.4028 - val_acc: 0.9032\n",
      "Epoch 74/100\n",
      "272/272 [==============================] - 0s 867us/sample - loss: 0.3196 - acc: 0.8529 - val_loss: 0.3973 - val_acc: 0.9032\n",
      "Epoch 75/100\n",
      "272/272 [==============================] - 0s 531us/sample - loss: 0.2997 - acc: 0.8640 - val_loss: 0.4000 - val_acc: 0.9032\n",
      "Epoch 76/100\n",
      "272/272 [==============================] - 0s 491us/sample - loss: 0.3216 - acc: 0.8419 - val_loss: 0.3930 - val_acc: 0.9032\n",
      "Epoch 77/100\n",
      "272/272 [==============================] - 0s 387us/sample - loss: 0.2817 - acc: 0.8860 - val_loss: 0.3956 - val_acc: 0.9032\n",
      "Epoch 78/100\n",
      "272/272 [==============================] - 0s 383us/sample - loss: 0.2902 - acc: 0.8750 - val_loss: 0.3964 - val_acc: 0.9032\n",
      "Epoch 79/100\n",
      "272/272 [==============================] - 0s 402us/sample - loss: 0.3106 - acc: 0.8676 - val_loss: 0.3910 - val_acc: 0.9032\n",
      "Epoch 80/100\n",
      "272/272 [==============================] - 0s 494us/sample - loss: 0.3049 - acc: 0.8676 - val_loss: 0.3889 - val_acc: 0.9032\n",
      "Epoch 81/100\n",
      "272/272 [==============================] - 0s 490us/sample - loss: 0.3150 - acc: 0.8566 - val_loss: 0.3952 - val_acc: 0.9032\n",
      "Epoch 82/100\n",
      "272/272 [==============================] - 0s 444us/sample - loss: 0.3114 - acc: 0.8640 - val_loss: 0.3992 - val_acc: 0.9032\n",
      "Epoch 83/100\n",
      "272/272 [==============================] - 0s 432us/sample - loss: 0.3024 - acc: 0.8676 - val_loss: 0.3992 - val_acc: 0.9032\n",
      "Epoch 84/100\n",
      "272/272 [==============================] - 0s 394us/sample - loss: 0.3319 - acc: 0.8640 - val_loss: 0.4062 - val_acc: 0.9032\n",
      "Epoch 85/100\n",
      "272/272 [==============================] - 0s 420us/sample - loss: 0.2909 - acc: 0.8971 - val_loss: 0.4097 - val_acc: 0.9032\n",
      "Epoch 86/100\n",
      "272/272 [==============================] - 0s 368us/sample - loss: 0.2985 - acc: 0.8640 - val_loss: 0.4142 - val_acc: 0.9032\n",
      "Epoch 87/100\n",
      "272/272 [==============================] - 0s 417us/sample - loss: 0.2758 - acc: 0.8603 - val_loss: 0.4080 - val_acc: 0.9032\n",
      "Epoch 88/100\n",
      "272/272 [==============================] - 0s 468us/sample - loss: 0.2907 - acc: 0.8640 - val_loss: 0.4138 - val_acc: 0.9032\n",
      "Epoch 89/100\n",
      "272/272 [==============================] - 0s 397us/sample - loss: 0.2923 - acc: 0.8676 - val_loss: 0.4098 - val_acc: 0.9032\n",
      "Epoch 90/100\n",
      "272/272 [==============================] - 0s 369us/sample - loss: 0.3117 - acc: 0.8750 - val_loss: 0.4100 - val_acc: 0.9032\n",
      "Epoch 91/100\n",
      "272/272 [==============================] - 0s 415us/sample - loss: 0.2937 - acc: 0.8750 - val_loss: 0.4123 - val_acc: 0.9032\n",
      "Epoch 92/100\n",
      "272/272 [==============================] - 0s 399us/sample - loss: 0.2774 - acc: 0.8640 - val_loss: 0.4180 - val_acc: 0.9032\n",
      "Epoch 93/100\n",
      "272/272 [==============================] - 0s 460us/sample - loss: 0.2759 - acc: 0.8934 - val_loss: 0.4220 - val_acc: 0.9032\n",
      "Epoch 94/100\n",
      "272/272 [==============================] - 0s 482us/sample - loss: 0.2881 - acc: 0.8787 - val_loss: 0.4260 - val_acc: 0.9032\n",
      "Epoch 95/100\n",
      "272/272 [==============================] - 0s 384us/sample - loss: 0.2933 - acc: 0.8566 - val_loss: 0.4288 - val_acc: 0.9032\n",
      "Epoch 96/100\n",
      "272/272 [==============================] - 0s 387us/sample - loss: 0.2789 - acc: 0.8787 - val_loss: 0.4364 - val_acc: 0.9032\n",
      "Epoch 97/100\n",
      "272/272 [==============================] - 0s 460us/sample - loss: 0.2901 - acc: 0.8971 - val_loss: 0.4388 - val_acc: 0.9032\n",
      "Epoch 98/100\n",
      "272/272 [==============================] - 0s 376us/sample - loss: 0.3156 - acc: 0.8750 - val_loss: 0.4305 - val_acc: 0.9032\n",
      "Epoch 99/100\n",
      "272/272 [==============================] - 0s 408us/sample - loss: 0.2857 - acc: 0.8787 - val_loss: 0.4243 - val_acc: 0.9032\n",
      "Epoch 100/100\n",
      "272/272 [==============================] - 0s 393us/sample - loss: 0.2779 - acc: 0.8860 - val_loss: 0.4349 - val_acc: 0.9032\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, Y, validation_data=(X_test, Y_test), epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd5zU1bn48c+zvbB9l7K7wC69CCyIiGIBKzY0mmvUqAkmMYkpatSovxjbvTfJvdcYk6iJxtgSa6zYRQN2mrB0lrYL23tvsztzfn98vzPM9gF2GNh53q/XvtiZ+c7M+TLwfeY855zniDEGpZRSwSsk0A1QSikVWBoIlFIqyGkgUEqpIKeBQCmlgpwGAqWUCnIaCJRSKshpIFBBRUSeFpH/8vHYAhE5y99tUirQNBAopVSQ00Cg1DFIRMIC3QY1dGggUEcdOyVzm4hsEpFmEfm7iIwQkfdEpFFEPhKRJK/jl4jIVhGpE5GVIjLV67HZIrLeft5LQFS397pQRHLt534pIjN9bOMFIrJBRBpEpFBE7u32+Cn269XZj3/Xvj9aRH4vIvtEpF5EPrfvWygiRb38PZxl/36viLwiIv8UkQbguyIyT0S+st+jVEQeFpEIr+dPF5HlIlIjIuUi8v9EZKSItIhIitdxc0SkUkTCfTl3NfRoIFBHq8uAs4FJwEXAe8D/A9Kw/t3+HEBEJgEvADfZj70LvCUiEfZF8Q3gH0Ay8C/7dbGfOxt4EvghkAI8BiwTkUgf2tcMXAskAhcAPxaRS+zXHWu39892m3KAXPt5DwDHAyfbbfol4PLx7+Ri4BX7PZ8DnMDNQCpwEnAmcIPdhjjgI+B9IB2YAHxsjCkDVgKXe73uNcCLxpgOH9uhhhgNBOpo9WdjTLkxphj4DFhtjNlgjGkDXgdm28d9C3jHGLPcvpA9AERjXWjnA+HAQ8aYDmPMK8Bar/e4HnjMGLPaGOM0xjwDtNvP65cxZqUxZrMxxmWM2YQVjE63H74K+MgY84L9vtXGmFwRCQGuA240xhTb7/mlMabdx7+Tr4wxb9jv2WqM+doYs8oY02mMKcAKZO42XAiUGWN+b4xpM8Y0GmNW2489A1wNICKhwJVYwVIFKQ0E6mhV7vV7ay+3h9m/pwP73A8YY1xAIZBhP1ZsulZW3Of1+1jgFju1UicidcBo+3n9EpETRWSFnVKpB36E9c0c+zX29PK0VKzUVG+P+aKwWxsmicjbIlJmp4t+40MbAN4EpolINlavq94Ys+YQ26SGAA0E6lhXgnVBB0BEBOsiWAyUAhn2fW5jvH4vBP7bGJPo9RNjjHnBh/d9HlgGjDbGJAB/BdzvUwiM7+U5VUBbH481AzFe5xGKlVby1r1U8F+AHcBEY0w8VurMuw3jemu43at6GatXcA3aGwh6GgjUse5l4AIROdMe7LwFK73zJfAV0An8XETCReRSYJ7Xc/8G/Mj+di8iEmsPAsf58L5xQI0xpk1E5mGlg9yeA84SkctFJExEUkQkx+6tPAk8KCLpIhIqIifZYxI7gSj7/cOBu4CBxirigAagSUSmAD/2euxtYJSI3CQikSISJyInej3+LPBdYAkaCIKeBgJ1TDPG5GF9s/0z1jfui4CLjDEOY4wDuBTrgleDNZ7wmtdz1wE/AB4GaoHd9rG+uAG4X0QagbuxApL7dfcD52MFpRqsgeJZ9sO3ApuxxipqgP8BQowx9fZrPoHVm2kGuswi6sWtWAGoESuoveTVhkastM9FQBmwC1jk9fgXWIPU640x3ukyFYREN6ZRKjiJyL+B540xTwS6LSqwNBAoFYRE5ARgOdYYR2Og26MCS1NDSgUZEXkGa43BTRoEFGiPQCmlgp72CJRSKsgdc4WrUlNTTVZWVqCboZRSx5Svv/66yhjTfW0KcAwGgqysLNatWxfoZiil1DFFRPqcJqypIaWUCnIaCJRSKshpIFBKqSB3zI0R9Kajo4OioiLa2toC3RS/ioqKIjMzk/Bw3T9EKTV4hkQgKCoqIi4ujqysLLoWmhw6jDFUV1dTVFREdnZ2oJujlBpChkRqqK2tjZSUlCEbBABEhJSUlCHf61FKHXlDIhAAQzoIuAXDOSqljrwhkRryO2OgpQqcnYf+GpHDILJbmfvOduhsg6iEw2ufUkodhiHTI/CrznaoL4Kmsl5/6op38ejDf+rzcZrKoK6wx8uef95i6vI3gfF173KllBp82iPwRUeL9WfaFAiP7vFwnaOAR59/ixv+32+73N/Z2UlYWBg0lkFjKbg6IeTAX/m7zz1q9Qg62iAipvvLKqXUEaGBwBcdrYBAWO87B95xxx3s2bOHnJwcwsPDiYqKIikpiR07drBz504uuer7FO7bQ1sn3HjTzVx//fXgcpJ1/Jmse++fNNXt4LzLvs0pp5zCl19+SUZGBm+++SbR0T2DjlJKDbYhFwjue2sr20oaBvU1pyW5uGdRKkjvmbTf/e53bNmyhdzcXFauXMkFF1zAli1bPNM8n3zy7yR3lNAanswJiy7gsssuI2WYV1DpbGXXrl288MIL/O1vf+Pyyy/n1Vdf5eqrrx7U81BKqd4MuUDgF65OCPc9dTNv3rwuc/3/9MhfeP1fL4CEUFhYwq5du0g5brz1YEgktLeRnZ1NTk4OAMcffzwFBQWDeQZKKdWnIRcI7rlo+uC+YGcbVGzvdWygL7GxsZ7fV65cyUcffcRX779GTKSw8PIbrLUAHS2AQFQ8NNYTGXmghxAaGkpra+tgnoVSSvVJZw0NpMO+IPfTI4iLi6Oxsfcd/+rr60lKSiImMZkdO3awatWqA68rAmFRgNGZQ0qpgBlyPYJB5/7mHh7V5yEpKSksWLCA4447jujoaEaMGOF5bPHixfz1r39l6gkLmZyVzvx5J4DLBZ12IHD3NHTLUKVUgGggGIijxQoCfQwUuz3//PO93h8ZGcl7770Hzg4o3wLxGRARC1U7KdixGaISSKWaLV9+6HnOrbfeOqinoJRS/dFA0B9jrBROdOLhv1ZouLWGwJ0SAqs3IGKlndxrFZRS6gjTMYL+OB1gnAc1Y6hf7gt+RytIKIRG2PdHW4vKND2klAoADQT9cX9LP4gZQ/0Kj7FmITmarN89PYMYwGU9ppRSR5gGgv54VhQPViCwX6ezvWtJCXePQ9NDSqkA0EDQn44Wa3pnyCD9NXmnmLx7GWGR1mB0h64dUEodeX4NBCKyWETyRGS3iNzRy+NjReRjEdkkIitFJNOf7TkoxlgzhiIGsd6Pe8AYugYFsXsdDu0RKKWOPL/NGhKRUOAR4GygCFgrIsuMMdu8DnsAeNYY84yInAH8FrjGX20aUKcDqnZaA8RgLfLyYaC4rq6O559/nhtuuKH/A93rBhwtnoHihx56iOuvv56YiBhoroTSjf2/RkMl1EZDUlbf5/D46b2WvVZKHeMW/wbmXDvoL+vP6aPzgN3GmL0AIvIicDHgHQimAb+wf18BvOHH9gyssw1cHRCVBKFhVromKmnAp9XV1fHoo48OHAgA4tKt97AHih966CGuvvpqYhLTAAH6mTlkXOAqg/zP+g4EFVuhYhtMuRASxw7cHqXUsSN1sl9e1p+BIAPw/lpaBJzY7ZiNwKXAH4FvAHEikmKMqfY+SESuB64HGDNmjN8a7CnzEDf8oKaMepehPvvssxk+fDgvv/wy7e3tfOMb3+C+++6jubmZyy+/nKKiIpxOJ7/+9a8pLy+npKSERYsWkZqayooVKwZonwHJg9Jc+uw4leRaf57zn5A8zudzUEoFr0AvKLsVeFhEvgt8ChQDzu4HGWMeBx4HmDt3bv+T7d+7A8o2H1prXB1WryA8tutK4pEz4Lzf9fk07zLUH374Ia+88gpr1qzBGMOSJUv49NNPqaysJD09nXfeeQewahAlJCTw4IMPsmLFClJTUwdun4iVUnJf7HtTuhEiEyApu+9jlApSpfWtXPn4Kv527Vwmjogb+AlHwMbCOn7+4gZeuv4kRib0XcrGn/w5WFwMjPa6nWnf52GMKTHGXGqMmQ38yr6vzo9t6p97Qddh7BH/4Ycf8uGHHzJ79mzmzJnDjh072LVrFzNmzGD58uXcfvvtfPbZZyQkHOI+xaHhVqmKvvZPLs2FUTMPrFFQapC4XIaqpvZAN+OwfLWnmoLqFj7ZWem392jrcFLf0uHz8c+t3se+6hY+3FbmtzYNxJ89grXARBHJxgoAVwBXeR8gIqlAjTHGBdwJPHnY79rPN/cBubeUHDnrkKeMGmO48847+eEPf9jjsfXr1/Puu+9y1113ceaZZ3L33Xcf/BuERli9lqo8GNGt5LazA8q3wok931upw7VsYwm/fGUT7910KuPThgW6OYdkS7G1adXWQd68ytt/v7Odj7eXs+K2hUSGhfZ7bHunk/e3WAFgxY4Krj0py2/t6o/fegTGmE7gp8AHwHbgZWPMVhG5X0SW2IctBPJEZCcwAvhvf7XHJ+4xgoP8Nu1dhvrcc8/lySefpKmpCYDi4mIqKiooKSkhJiaGq6++mttuu43169f3eK5P3GUpeptdVLHdKosxKueg2q+ULzYW1eFwunjmy4JAN+WQbS2p7/KnP3y5p4qS+jbe2VQ64LGf7qyioa2TSSOG8eWeato6emTGjwi/riMwxrxrjJlkjBlvjPlv+767jTHL7N9fMcZMtI/5vjEmsP1O47JqAB1kIPAuQ718+XKuuuoqTjrpJGbMmME3v/lNGhsb2bx5M/PmzSMnJ4f77ruPu+66C4Drr7+exYsXs2jRIt/eLCTMGsPobZzAHRw0ECg/2FPZDMArXxdR3+p76uNoYYxhW2kDIQK7K5podQz+Rbe+pcPz9/TUFwWYAeqHLdtYQlJMOHecN4X2Thdf7a3u93h/CfRg8dHF5Ryw3HRfupehvvHGG7vcHj9+POeee26P5/3sZz/jZz/7me9vJGINXpf2FghyISJOZwupw7KxsI51+2r53ildJxzsqWhi8og48sob+de6Qr5/qvXvrKKxjcc+2cuNZ00kPir8sN77jQ3FxESEcs70kYf1Or0prGmlsa2ThZPTWJlXyY6yBmaPGXh6+MHYWGQNcZ533Eje21LG1/tqmZuV3OuxLY5OPtpWzqVzMjh5fCrR4aGs3FHBosnDexzb6nDy8xc38LMzJjAzcxCqIXejJSa8GReE9J/TOyqk51gzo1zdvtGU2APFg1USQwWl//1gB//1zjaa2g9MSGh1OCmua+WCmaOYl5XM018W4HQZmts7ue7ptfz983yWby0/7Pf+vw/yuPmlXCoaB78A4xY7HfStudYcFn+ME2zYX4cI3LtkOvFRYTz1RUGfx360vYLWDidLZqUTFR7KggkprMir7NGLcLoMN720gY+2l1Na75/ClHrF8GYOvUdwRI2aZdVBqtp14D5npzWbSNNC6jBUNLbx1Z5qjIHtpQculHsqrTGv8WnDWLogi6LaVj7cWsaNL25gW0kDUeEhrDrMtEZbh5OS+laaHU7+7/28w3qt3mwtqSc0RFg0ZTgJ0eF+CQS5hbVMSBvGiPgorpg3hve3llFS13sNsWW5JYyMj+IEu8ewcPJw9te0sLequctxv313Ox9sLeeuC6Zxrh96SjCEAsFAuTifuFxHdSDwnKP7Yu89YFyVZ80mStdAoA7dO5tKcdn/zLYWHxhQdQeCCcOHcfa0EWQkRvOLlzfy0fYK7lsyndMnpbE6v6bH6zk6fd+Lu7CmBWMgOzWWf31dxMbCwZ1JvrWkgYnDhxEVHsr09PhBHzA2xpBbWMfsMVbq5tqTxmKM4R+r9vU4tr6lg092VnDhzFGEhFhjkgsnpwHW7CG3Z74s4InP8/nuyVlctyBrUNvr7ei96h2EqKgoqqurDz8YuAeLj0LGGKqrq4mKioLUSVaROu9xAs9A8azANFANCW9tLGHKyDhSh0V0+ca8p7KZEIGxKTGEhYZw7Uljae1w8oNTs7nmpCxOzE5hf01Ll2+/5Q1tzL7/Q97bPPDsGYB8+5vw/RdPJ3VYJPe9tXVwvuDZthQ3MD3dWr8zPT2eHWWNdDh9D1QD2VfdQm1LBzmjrXGHzKQYzpk2khfX7Kez2/t8uK2MDqfholnpnvsyk2KYNGIYK/MqqW5q587XNnPvW1s5a+pwfn3hNMSPa4OGxGBxZmYmRUVFVFYe5iKRhhKrJHTF0bloJioqiszMTKsO0sjjus4cKsm1ZhOlTAhcA4NUYU0LkeEhDI8LzKrQQ1XV1E5bh5PMJKucSmFNC+v313HbuZNZk1/DFu9AUNHE6OQYosKtL0rfOyWb6ekJnDw+BYATx1npjdX51XxjtlVE+K2NJTQ7nCzfXs55M0YN2J6CaisQzMhI4JeLJ/PLVzbxZm4Jl8zOOOxzrWhoo6qpnenp8QAcl5GAo9PFnsompoyM9/l18quau6TM5mYleT73XLsH4+4RAFyck877W8vYUFjnSQEBrMirYGR8FDMzuy4sXTR5OE9+kc/CB1bS6nCy9ORsbjt3MqEh/l0gOiQCQXh4ONnZg1BS4X8vhGlL4MI/HP5r+duoHNj4gpXOCgmxegQjZxwbg91DzPeeWUt2aiyPXTM30E3xmTGG655eS35lM//68UlMGRnPW5tKAFgyK53m9k6++HQv7Z1OIsNC2VPZxASvRWRhoSGcMvFAWZSpI+NJiA5n1Z6aLoEAYPXenimj3uRXtZAYE05iTATfnJPJP1ft455lW5mRmXDYC9jcvRt3IHD/uaW4wedAsLmonssf+4pWr7n+o5OjWX7z6USFh7Jhfy0xEaFM8ipdsWBiKmEhwoodFZ5A0OF08dnOKi6cNarHt/xzjxvJ45/tZc6YJH594TQmDD8yC/eGRGpo0DiaICI20K3wzahZVnu/ehjW/8OaRTSExgc+3l7O5iL/LfoZLLXNDnaWN7GzvCnQTenT+1tK+Xpf14vxun21bCqqp7XDyXVPraW8oY23NpYye0wio5NjmJ6eQKfLsKu8CafLsLeqmfH9XJRCQoQTspJZnW8NGBdUNbOxqJ7s1FiK61oprBl4r42CqmayUmI9r/fwlXMICxGWPrX2sEtbbLHHO6bZASA7dRjR4aE+jxMU17Vy3TNrSY6N4I2fLOCDm07jj1fkUFjTyt8/zwesHsHMzIQu397jo8I5fmwSK/IOZCu+3ldLY3snC3uZJjpnTBLr7zqbZ66bd8SCAGggOMDZaQ22RhwdhagGNOYka2B7+a9h2U+hoxnGLgh0qwbN7a9u4n/e3xHoZgwo1543XljT0iMPfDSobmrn5y/m8uN/ru8yHfSpL/KJjwrjxevnU9fawRWPr2J7aQNL7Jz1gW/M9RTVtuDodHXpEfRm/rhkCqpbKKtv4227d3H3RdMAfJpRVFDdTHbqgS9iY1JieOI7cylvaOP7z6w7rFW3W0sayEqJIc5e5xAaIkwZFefTzKGGtg6WPrWGtg4nTy09gZzRiUweGcfFORmcO30Ej6zYzf7qFraVNnjGB7wtmjKc7aUNlNlTP1fkVRAeKiyY0HuhyaTYiEM+z0OlgcDNYX+jizxGaqikToDb9sBNW6yfW3Zaaa0hoLbZQVWTg42FdbhcgzdYeLCcLsMvXsrlqz19X8Ry91uBoNNlKKrte6vRD7eW8dt3tx9SO8ob2rjpxQ3srz74HexeWLMfR6eLisZ2Hl2xG7C+3X6wtZwr541hblYyj1w1h33V1mDwBTOtXP6Y5BjiIsPYWtJwYOro8P57y/PHWeMFq/OrWbaxhHlZyZw+MY2kmPBeZxR5a3U4Ka1v8/QI3GaPSeKPV+SwsaiOcx/6lEse+YJLHvmCe5f1PZDsdBleWrufnzy3ni92VwGwtbTeM1Dsdlx6AttKGvr9N7a3sonrnlrL3spm/nr18V3SPgC/On8anU7DD//5NR1O02V8wM29QGxlnjUbaOWOSuZlJzMs8ujJzGsgcHMHgmMlNQQQkwyJo62fuBGBbs2g2VtlfRaN7Z2e3wMht7CO1zYUc/urm2jv7P3baG5hHWF2KiC/urnXYwCe/CKfxz7dS3nDwS8I+s2723kjt4R7lm05qOd1OF38Y9U+Tp2YyqWzM3jis3z2V7fw7FdW6YNrTrI2Llo0ZTgPXzWHO8+b6hn4DAkRpqbHs6Wknj0V1nkNlKefOiqeuKgwnv1qHzvLm7goJ52QEGFe9oGUUV/21VjvkZXacx+QxceN4vf/MYuxKbHER4cTIvD0lwW81Ustn7UFNVz8yOfc/upm/r2jgm8/sZrvPb2WwppWpmd0HQuYnh5PU3sn+3tJW9W3dvBfb2/jnD98yo6yRh78Vk6v3+DHpMTw/VOzPQPIs0f3DASTRgwjPSGKFXkVFNe1klfe2Ovq4UDSQODW7g4Ex0iPYAjbXXHg4r9+v29zyZ/4bC9LHv58UKcbur/B7a9p4cnPC3o87p437s71FlT1HgjaOpxssM/jk7yDm9m2rqCGN3NLGJ8Wy4q8SlbkVQz8JNu7m0spb2jnugXZ/HLxFMJChXuWbeHFNYWcO32kZ7YQwPkzRvGD07qWJpmeHs+O0kbyyhtJHRZBYkz/KYvQEGFeVjJf76slNEQ4/zhr8dP8cSkU1rRSbE8tbXF0cu4fPuWpL/I9z3X/3XmnhrxdOieTZ6+bx7PXzeNfPzqZ6enx/O7d7V3qBT32yR7+469fUd3k4I9X5LDh7rO57dzJnvo9x3XvEWRYt//vwzzPSmany/DCmv2c8cBK/v5FPpfNyWTFrQs9KbPe3LBoAsPjIslIjGZ4fM+ZYyLCwinD+XxXFcu3WpVGexsfCCQNBG4O+z9x5DEyRnAM8GWAsLyhrceioz2VzUSEhRAXFeaZktcfYwzPfrWPTUX1PXK+TpfxqR29WZFXwQlZSZw1dQQP/3tXj7IH+VXN1Ld2cPa04QyLDOszEGwsrKPdPseDuZC7XIb73trGyPgoXvvxArJTY/nPt7f5PPf9qS8KGJcay+mT0hiZEMVPFk1gRV4l9a0dLF0w8Cy76ekJtHY4WbGjgnE+ztpxTyNdMCGVlGGR1n3ZdsrIviD/deUe8sobeW39ge1J8quszyirj0DgLTREuOei6ZTUt/HYp3sAa4bSb9/bwQUzR/HvWxZycU4GUeGh1jnfupD//ebMHt/op6fHc8PC8Xy4tYwzHviEBz7I46I/f86dr21mXFosy35yCv/zzZmkxUX2255hkWH8/Tsn8ODlfa/hWTR5OM0OJ4+s3MPo5GjGpx1dmQcNBG4OuxT0sZQaOoqt31/Lqf+7gi/3VPV5TFuHkzN//wl/+2xvl/v3VDQxLjWWnNGJnm/S/cktrPN071d2u9A++1UBCx9Y2aWX4YuKhja2FDewcPJw7rpgKg6nq0fZA3fbckYnkZUaQ34fOfzV+TWIwPkzRvLZriqfL+SvrC9ic3E9d5w3hYSYcO66YCp7K5t59queK1W727C/ltzCOr5zcpZn5er3TslmTHIMMzISOCFr4GJrx9mplOpmh88zWE6ZYK2OvdRr7v+UkXHW1NK91RTWtPDYp3sZFhnG5uJ6T3AtqGomJTbC56J187KTuXDmKP76yR7ezC3mln9tZF5WMg9ePovoiK5TqEfER3H53NE95uKLCL9cPIUPbz6d+eOSeXjFbupaHPz5ytm8/MOTmJHp++ZRMzITONEeI+nNyeNTiAgNobKxnUWTh/t1cdih0EDgpqmhQfWpvQPUqn7mkOeVNdLU3tljRsnuyibGDx/G7NGJ5JU10OLoYzc221sbS4kIDfGkT7y9tr4Yp8vw9Jf5fTy7dyvt9i+aPJys1FiuOyW7R9mD3MI6hkWGMWH4MLJSYvvsEazaW83UkfFcnJNBU3sn6wpqPY/VtTh4f0tZj5RWY1sH//t+HrPHJHJxjpWWOGPKcE6blMZDH+2kup/plE6X4bFP9hIXGcZlx2d67o8KD+XVH5/M00tP8OlCND5tGBFhIZ7ffTEtPZ6PfnGap82A1zhBDb97bwci8NC3rKnOn+60vijkVzf71Bvwduf5UzEGbnwxl8zEaB675vgBN4LpTXZqLE985wRW3rqQj29ZyEWz0gf9Qh0bGebpLR1t4wOggeAATQ0NKvciog37a/s8xl0NMnf/gdlBbR1OCmtaGJ82jJwxibgMbOpnPYHTZXh7UwkLJ6dxwcx0NuyvpbbZAVipm83F9cRHhfHq18UHtX3gyrwKRsRHMnWU9e/hp4sm9Ch7sKGw1jNvPDs11jPN0puj08X6/bWcOC6ZBRNSCQ+VLr2W21/dxI/++TWP2DN6ADqdLn7+wgZqWxzcc9F0z0VJRLj7wqm0OJz8fvnOXtu9Jr+GJQ9/zvtby7j25LE9ZqakxUV6UjYDCQ8NYcpI6/wPJpUxYXhcjwvpidnJ7Ktu4Z3Npfz49AmcOXU4w+MiPaky7zUEvspIjOaWcyaRkRjNU0tPOOxpl1mpsT16E4PpkpwMMhKjPbOrjiYaCNw0NTRo2judrLcDQH9TQN35fO/ZQfuqW3AZq7iZe052f+MEq/OrqWhsZ0lOOosmp+Ey8Oku69v8slxrLvtDV+TQ2uHkpXX7fWq/e+Wndxc+LiqcXy6ezPr9dSzbWEJbh5MdpY2e6YJZKbG4DBTWdk0PbSqqo63DxYnZKQyLDGNedrLn4vfF7io+2FpORmI0D3y4kzc2FGOM4Z5lW1mRV8l/XnwcOd1moUwYHse1J43lhTX7uyyG6nC6+MVLuVz+2FfUNDv405WzufWcyT6db3/c6wkOd3GT++KXkRjN9aeNQ0RYNHk4n+60xiwqGtvJ7mXG0ECuP208n9++iLEHGUQC4bLjM/nijjP8GmwOlQYCN3ePQFNDh21jYT3tnS7OmjqChrbOPqdVbi1pYKQ9y8I9O8idyx+fFktybARjU2L67VW8tbGEmIhQzpwygpmZiSTHRrDSrum+bGMx87KSOWPKCE7MTuaZL/f5tOirr5Wf35yTyYyMBH777g7W5NfQ6TKeYOVOa3RPD7nTXidmH0gL7CxvYn91C/e/tY3MpGjev+lU5o9L5pevbOL2Vzfx3Or9/Oj08Vx14phe23fTmZNIjA7n/re2YYzBGMP/e20zr20o5ieLxjXEqRUAACAASURBVPPvW6xZLoOR3vjG7EwunZ1BekL0Yb3O1FHxXDBzFL+9dIbnQrhoShqNbZ28vr4I8G2guDdHW779WKSBwK29CRAIP/hvJaqr1XurEYEfnm5NR+xtwLfT6WJHaQPnzxjVZXbQnsomRGBcqhWQZ9sDxr1NC3V0unhvSxlnTxtBdEQooSHC6ZPS+GRnpb0QqpmL7Fz10gXZFNe18tH2gTdPWZlXaa/87NqFDwkR7l0yjbKGNu58bTOA5xu7e9pjfrdAsDq/hikj4zxpC3dw+fmLG8grb+SuC6YSFxXOY1fPZXRyNC+vK+KCmaP45bl9f5tPiAnnlnMmszq/hve2lPHwv3fzr6+L+PmZE7nt3CmD+o1zXnYyD34rxzPgfKhCQ4RHrprDaZPSPPctmGDV4XnGHvw+2NSQGjwaCNzcdYZ0d6/Dtiq/mskj4jh+TBJxkWHkFvb8Rr+nspn2ThczMxO6zA7aU9lERmK052KWMzqRisb2Xndm+nx3JXUtHV3meC+cnEZNs4PfvLu9y1z2s6eNIDMpmif72THKbWWeVSAsrpcZLMePTWbJrHSK61rJTIr2TC1MigknPirMU0ETrHTNuoJaT28ArJ7O6ORocgvrOGlcimejkYSYcJ793oncvngKv/+PWQNeeK84YTRTRsZx52ub+f3ynVw6O4Obz5o44LkdTeKiwjkhK9kTPA+1R6AOn1713BxNmhYaBI5OF1/vq2X+uBRCQoRZfUwBdee3p6fHk+M1O2h3RVOXGSruPWV7Gyd4fvV+EqLDOXXigW+Zp01MI0Tgyz3VnOI1lz00RPjOSVmsya/pUYDN2yc7K9lR1v/KzzvOm0JUeAjHjz0wBVPEGjAuqDowRuAu6uY9rVBEOHPKCELEqsPjndbISIzmxwvHe0o99ycsNIS7L5pGfWsH88cl87vLZh6TKZJFU6zPLi0u8qgquRBsNBC4tR/9lUddLsOXe6oGdfVsfyob23l9QxGvrbd+1hYMXE54c7E1ODrfniqXMzqRHWWNXVaAgjU+EBUewri0Ycy2ZwdtLKxnb2Vzl0AwdVQ8EWEhPcYJPt9VxUfbK7j+tHGeKY5gFexyB4/uq0GvOnEMI+Ijue+tbb0OYG8raeAnz61n6qh4ruwjPw+QnhjN6zcs4FcXTO1yf1ZqbJfUkHt8YF52183Lbz5rEq/fsICpo3yvg9+bk8en8sZPFvDkd0/o8ndwLHEH3GxNCwXUsfmvxx8cTUd9wbnPdldx1d9W8+G2w98kfCDVTe1c9pcvufmljfziZevnisdX9bn/qpt73cA8ezXp7DGJOF2GzcVdp4BuKa5nysh4QkOEWZlWnv29LaW0dji7zFCJCAvh+DFJvPJ1keci2+l0cf/bWxmdHM33Tum5Qvb8GaNIiA7nnOld6y/FRoZxx3lT2FRUz6v2AKVbWX0b1z29lmGRYTz13RMG/HY6dVR8j41oslJiKalvpa3DSYfTxesbipk6Kp7UbtM1E2LCmdVLTZpDkTM6kZiIY/eb9IThw5g8Io5Zo31fvKUGnwYCN0fzUV+COq/Mmm7pnhbpixfW7GdneeNBvU9bh5PvP7uO8oY2nl56Ap/ctpDXbji51/1Xyxva+MPynZTWWwFi1V5rfCDZHhx1D6Z6jxMYY9hW2uBZuZoyLJKxKTG8aZ9X9znrv710BgBLn1pDTbOD59fsZ2d5E786f1qvaZSlJ2fx1Z1n9Jrjv3hWBrPHJPI/7+fR2GatKyisaWHp02tpau/kqaUnMDLh0HYay0qNwRjr9Z5btY/dFU384uxJh/RawUJEePOnC7jjvKkDH6z8RgOBW3vjUZ8ack+t/HhHeZfa8n1x73v6p493+fweLpfh5pdyyS2s449X5LBw8nDGpsQyZ0wSZ08bwQtr9ndJ89z1xhb++PEuznjgE/708S6+3lfrWUEJ1kV+THJMl3GCwppWGts6u5QFzhmdSH2rdWHuvgFKVmosT3xnLiX1bXzvmbU8uHwnJ49P4dzpvVdcDQmRPr8lh9h1aqqa2vn9hzv5vw92cOaDn1BQ1cwj355zWOka96yXDfvreHD5Tk6dmMpZU4++VaRHm6jwUL9vxaj6p4HALcCpoS93V3HzS7n95v/3VDaTEB1OW4eLj3xID62xa8B/urPS501T/vzv3by3pYxfnT+Vxcd13Wd26YJs6lo6eCPXKhb22a5Klm8r57oF2SycnMaDy3fS4nB6ioy55YxO7DLYu8VroNjNXb43MSaclF5WiB4/Npk/XJ7Dhv11NLR29BhoPRg5oxO5bE4mT39ZwCMr9nDBjFH8+9bTOd1rauOhcE8h/c1722l2OP2+4bhSg+XYTS4ONkdzQGcNPb9mP29vKuWWcyZ1KQ/sZoxhd0UT588YySd5lSzbOPCm3u7Byoa2zh6bZ/emqb2TJz7by3nHjew1935idjJTR8Xz1Bf5fPP4TO5/axtjU2K4/bzJRIaF8uWeKj7cWu6ZCeI2e0wiyzaWUFbfxsiEKLaW1BMaIl02+XAP8I5PG9bnxfOCmaPocObQ1uE8qA3He3Pn+VOIjgjhG7Mzu8z+ORyJMREkxoRT19LBd04a22MTE6WOVtojcGsP3PRRY4xnB6e+8vnVzQ7qWzuYMDyOi2al8+nOSk9Nnb6szq8hZ3SiZ/Psgbz6dRGN7Z2eEgDdiQhLF2Sxs7yJm17MZVdFE786f6qn0NfJ41O5d8n0HmkZ90X+3mVbKa1vZWtJAxOHD+uS3586Kp7IsBAmDlDK4JLZGVwxr+8ZPb5KHRbJf10yY9CCgNv4tGEkxoRzs44NqGOIBgIAl8va8zdAqaG9Vc1UNlrVJPPKei+XvMceH5gwfBgXzUqn02V4b0tZn69Z0+xgR1kjZ08b0WPz7N64XIanvywgZ3Si58LdmyWz0kmJjeCdzaWcMiGVs6cNvDParMwEbjprIv/Oq+CMBz5h9d6aHtsGRoSF8PTSefz8zGNrUVR3/3nxcTx73bwBN3FR6miigQCsIAAB6xG4K3VGhoV4ZgZ1t6fSvV1gLNPT4xmXGsuyjcW9HgsHxgdOzE7usXl2bz7ZWUl+VTNLF2T129ao8FCuOWksYSHicw5cRLjprEl8/IvTWTQljdYOZ6/18E8an0J64uHVtAm0aenxzMwcnKmhSh0pGgjAay8C/8wa2lJc76nG2ZtVe6sZHhfJ/HEp5JX33iPYXdFEdHgo6QnRiAgXzUpndX4Neyt7P37V3mqiwkOYmZnYY/Ps3jz5RT4j4iM5f8aoPo9x++miCay8bSGTRx5cDnx0cgyPfvt4vrjjDC6fO/qgnquU8h8NBOD3vQj+651t3PV67xuPW+MD1Zw4LoUpI+PYU9HU6wyfPZVNjEuL9dSguXROBlFhoZz/p8946KOdPVburs6v4fixSUSEhXTZPLs3uysa+WxXFdfMH0t46MD/JMJCQ3od0PZVRmL0YRcxU0oNHp01BF57EfgnNVRY09rnLlv7qlsob2hn/rhkosJCcThdFFQ3M2F416C0p7KJOV65+7EpsSz/xWn89r0dPPTRLl5eW8hfrzmemZmJ1LU42FHWwM1nWQOW7s2z39xQjKPTRWNbBw+v2O1Zl1Bc10pEWAhXDsIgrFLq2KM9AvBraqjT6aKsoY3alg7aOpw9Hj9Qrz7Fk2rpPmDc6nBSXNfaY3OQzKQYHrlqDi9dPx8R4bqn11JY08Ka/BqMoctOSO7Ns3/9xhYWPrCSZ7/aR0NbJ03tnSREh3PL2ZN83rlKKTW0aI8AvFJDg98jKK1vw2kXOCurb+tRand1fg2pwyIZnxZLe6eLELFKSVww80Cufm9VE8b0vW/sieNSeOa6eVz66BcsfXotOaMTiQwL6VK/xb159kvrCjltUhp3Xzi1R69DKRWcNBCAtaoY/FJrqNirSFtpt0BgjGHV3mpOHJeMiBAVHkpWSix53dYSeHbtGt53j2XC8GE8fu1crvn7anZXNHHSuJQuG3nHRobxpytnExUewumT0nTFq1LKQ1NDYNUZAr+khopqDwSCsoaulTsLa1oprW9jvleZ4skj49jZbebQnspmQmTgHZzmj0vh/745C7B6AN0tPm4kC7324VVKKdAegcWPqaEir83My+rbuzy2Kt8eH/DK5U8aEcf7W8todTg9u3TtqWxidHKMTxuWXDI7g/Fpw/rtPSillDftEcCB1FD44F88i2tbGREfSXxUGGX1XXsEuYV1JESHdymrMHlkHMYcSAeBtap4Qh/jA72ZkZlwTNeoV0odWX4NBCKyWETyRGS3iNzRy+NjRGSFiGwQkU0icr4/29On9kYIi4bQwb94FtW2kpkUw6iE6B777uZXNjMuLbZLqsYzc8geJ3C6DHurmnuUZlZKqcHit0AgIqHAI8B5wDTgShGZ1u2wu4CXjTGzgSuAR/3Vnn45/FdnqKiuhYzEaEYkRFHW0DUQFFQ399iib2xyDBFepSaKa1txdLp6bNailFKDxZ89gnnAbmPMXmOMA3gRuLjbMQZw1xNOAHzfemswOfyzX7HTZSitayMzKZpR8VFdav20Opw9ZhGBtWp3Qtow8sqbcHS6eH7NfoAeawiUUmqw+DORnAEUet0uAk7sdsy9wIci8jMgFjjLj+3pW3uTX6aOlje00ekyZCbFEB4aQmVTOx1OF+GhIeyrsQaouwcCgCkj4/hoeznnPvQp+VXNnD1thBYyU0r5TaAHi68EnjbGZALnA/8QkR5tEpHrRWSdiKyrrOy/nPIh8dPuZO6poxlJ0YxKiMIYqLDLTRfYG7F3Tw2BVZu/oa0TEXhq6Qn87dq5PtUAUkqpQ+HPHkEx4F1iMtO+z9v3gMUAxpivRCQKSAW6VEczxjwOPA4wd+7cvvdyPFSOJojpOe/+cLmnjmYmReMy7tXFrWQkRpNfZT2WldqzeNu3548hKzWWhZPTNAAopfzOn1eZtcBEEckWkQisweBl3Y7ZD5wJICJTgSjAD1/5B+Cn3cmK3T2CRKtHAAfWEhRUNZM6LIK4qPAez4uJCOPsaSM0CCiljgi/XWmMMZ3AT4EPgO1Ys4O2isj9IrLEPuwW4AcishF4Afiu6W/3dn/x06yhotpW0uIiiQoPZVS8teFKqb2WIL+6ecCVwkopdST4ddWRMeZd4N1u993t9fs2YIE/2+ATh396BO6powDx0WFEh4d6Zg4VVDVz2qS0/p6ulFJHhOYejPFbICiubSUzyQoEIsLIhChKG9pobu+korGd7F5mDCml1JGmgaCjFYxr0FNDLpehuK61y05eI+OjKK9vo6DanjqqqSGl1FFAA4GnBPXhB4KlT63hv9/ZBljTRDuchoykA5uxj0qIorS+jYJ+ZgwppdSRppXJBikQlNa3siKvkpU7K7k4J8OzG1mmVyAYmRBFeUObZ8N57REopY4G2iMYpG0qV+ZZs16jw0O5/61tnsVko7sFgk6X4ev9tQyPiyQ2UuOwUirw9Erk7hEc5hjBih0VZCRGc8Oi8fzq9S20dVo9gozErmMEAGvza5iekdDr6yil1JEWnIGgeg98dC84O6ClyrrvMGoNtXc6+WJ3FZfMzuCKE8bwz1X72VRUT0pshGdzGYBRCVbvoNnhJCtFxweUUkeH4EwNbXkVti+DhmLobIesUyFtkk9PNcbw4PKdbC6q99y3rqCWZoeTRZOHExoi3HORVW3be3wArNSQW2/F5pRSKhB86hGIyGvA34H3jDEu/zbpCCjdCCkT4EefHfRTi+ta+dPHu3hvcynv3ngq4aEhrNhRQURoCCdPsOoVzR+Xwg9PH+dJBbmlxEYQHip0OE2vxeaUUioQfO0RPApcBewSkd+JyGQ/tsn/SnJhVM4hPTW3sA6AXRVNPLdqHwAr8io4cVxyl+0h7zxvKksXZHd5bkiIMDzOCg7aI1BKHS18CgTGmI+MMd8G5gAFwEci8qWILBWRnlXTjmbNVdBQBOmHFgg27K8jMiyE+eOS+cNHu9hUVMeeymYWTR7u0/Pdxed06qhS6mjh8xiBiKQA3wW+D2wA/ogVGJb7pWX+Uppr/Tlq1iE9PbewjhkZCdy35Dga2zq4/tmvAVg0xbdAMDo5hsyk6C6DyEopFUi+jhG8DkwG/gFcZIwptR96SUTW+atxflFiB4KRMw/6qY5OF5uL67l2/lgmj4zj6vljefarfWSlxPhcN+iO86ZQ39px0O+tlFL+4uv00T8ZY1b09oAxZu4gtsf/SjdCUjZEH/zWjzvKGnB0upg9JgmAm8+axLubS1l83CifX2NEfBQjug0iK6VUIPkaCKaJyAZjTB2AiCQBVxpjHvVf0/ykNBcyjj+kp27Ybw0U54yxgkhSbAQrbl1IdLimeZRSxy5fxwh+4A4CAMaYWuAH/mmSH7XUQN3+w5oxNDwuknSv9QBxUeGE6U5iSqljmK9XsFAREfcNEQkFIvzTJD8q3Wj9eYgDxRv215IzOhGvvwqllDrm+RoI3scaGD5TRM7E2lbyff81y08OY8ZQbbODguoWz/iAUkoNFb6OEdwO/BD4sX17OfCEX1rkTyW5kDgGYpIP+qnuhWQ5ow9+kFkppY5mPgUCu6zEX+yfY1fpxkMeH9hQWEeIwMxMrRqqlBpafEoNichEEXlFRLaJyF73j78bN6ha66A2/zBWFNcyaUSc7iGglBpyfB0jeAqrN9AJLAKeBf7pr0b5Rdkm689DGB9wuQwbC+uYPUbTQkqpocfXQBBtjPkYEGPMPmPMvcAF/muWH7hXFB9CamhTcT0NbZ3MHXvwYwtKKXW08zXP0S4iIVjVR38KFAOHv9v7kTT5PGs1cWzqQT91WW4JEaEhnDVthB8appRSgeVrILgRiAF+DvwnVnroO/5qlF+kTrR+DpLTZXh7UwmnT04jIfrYKrSqlFK+GDA1ZC8e+5YxpskYU2SMWWqMucwYs+oItO+IKqtv49w/fMqW4gO7j63Or6aisZ0ls9ID2DKllPKfAQOBMcYJnHIE2hJw28sayCtv5O43t2CMAeCtjaVEh4dy5lTfykwrpdSxxtfU0AYRWQb8C2h232mMec0vrQqQBrs89Pr9dSzbWMJ5x43ivS2lnD1tRJfdx5RSaijx9eoWBVQDZ3jdZ4AhFQjqWqxAkJ0ay2/f3UFoiFDX0qFpIaXUkObryuKl/m7I0cAdCH576QyueHwVt7+yifioME6blBbglimllP/4ukPZU1g9gC6MMdcNeosCqK7VQVxkGPPHpbBkVjrLNpbwrbmjiQjTMtNKqaHL19TQ216/RwHfAEoGvzmBVd/aQUKMNUX0zvOnsK+mhWtOGhvgVimllH/5mhp61fu2iLwAfO6XFgVQfUsHiXYgGJUQzZs/WRDgFimllP8das5jIjDk5lPWtXboojGlVNDxdYygka5jBGVYexQMKXUtDqaMjA90M5RS6ojyNTUU5++GHA3qWzs9YwRKKRUsfN2P4BsikuB1O1FELvFfs448Ywz1rQ4SNTWklAoyvo4R3GOM8RTgMcbUAff4p0mB0eJw0uE0OkaglAo6vgaC3o4bUjUX6uzyEomaGlJKBRlfA8E6EXlQRMbbPw8CX/uzYUdavb2qOCE6IsAtUUqpI8vXQPAzwAG8BLwItAE/GehJIrJYRPJEZLeI3NHL438QkVz7Z6eI1B1M4wdTXasD0B6BUir4+DprqBnocSHvj72PwSPA2UARsFZElhljtnm97s1ex/8MmH0w7zGYDvQINBAopYKLr7OGlotIotftJBH5YICnzQN2G2P2GmMcWD2Ji/s5/krgBV/a4w86RqCUCla+poZS7ZlCABhjahl4ZXEGUOh1u8i+rwcRGQtkA//u4/HrRWSdiKyrrKz0sckHx115NFHHCJRSQcbXQOASkTHuGyKSRS/VSA/DFcAr9m5oPRhjHjfGzDXGzE1L809J6PrWDiLCQogK10qjSqng4usU0F8Bn4vIJ4AApwLXD/CcYmC01+1M+77eXIEPg8/+VN/qICE6HBEJZDOUUuqI8+nrrzHmfWAukIeVx78FaB3gaWuBiSKSLSIRWBf7Zd0PEpEpQBLw1UG0e9DVtXToqmKlVFDytejc94Ebsb7V5wLzsS7cZ/T1HGNMp4j8FPgACAWeNMZsFZH7gXXGGHdQuAJ40bh3iw+QOq8S1EopFUx8TQ3dCJwArDLGLLK/xf9moCcZY94F3u12393dbt/rYxv8qr61g/TE6EA3QymljjhfR0bbjDFtACISaYzZAUz2X7OOvHrdi0ApFaR87REU2esI3gCWi0gtsM9/zTry6locmhpSSgUlX1cWf8P+9V4RWQEkAO/7rVVHWIfTRbPDqYPFSqmgdNAVRI0xn/ijIYFUr6uKlVJBTFdPcWBVcbz2CJRSQUgDAdZiMoDEGC0voZQKPhoI8K4zpD0CpVTw0UCAjhEopYKbBgIO9Ah0HYFSKhhpIMDai0AE4qI0ECilgo8GAqC+xUF8VDihIVp5VCkVfDQQYI0R6PiAUipYaSDASg3p+IBSKlhpIMAaLNZAoJQKVhoIcKeGdDGZUio4aSDADgTaI1BKBamgDwQul6GuxaGpIaVU0Ar6QNDk6MRldFWxUip4BX0gqNdVxUqpIBf0gcBTcE4Hi5VSQSroA4G74Jz2CJRSwSroA0F5QxsAqcO0R6CUCk5BHwgKqpsJDRFGJ8cEuilKKRUQQR8I8quayUyKJjw06P8qlFJBKuivfgXVzWSlxAa6GUopFTBBHQiMMRRUtZCdqoFAKRW8gjoQVDU5aGrvZGyKjg8opYJXUAeCgupmALK0R6CUCmJBHQjyq6xAkK1jBEqpIBbUgaCgqpmwECEzKTrQTVFKqYAJ7kBQ3czo5BjCdOqoUiqIBfUVML+qhSwdKFZKBbmgDQTGGPZVN+tAsVIq6AVtIKhobKfF4dQ1BEqpoBe0gcA9Y0hXFSulgl3QBoIC99RR7REopYJc0AaC/OpmIkJDSE/UqaNKqeAWtIGgoKqZ0cnRhIZIoJuilFIBFcSBQIvNKaUU+DkQiMhiEckTkd0ickcfx1wuIttEZKuIPO/P9ri5XEbLTyullC3MXy8sIqHAI8DZQBGwVkSWGWO2eR0zEbgTWGCMqRWR4f5qj7eyhjbaO126hkAppfBvj2AesNsYs9cY4wBeBC7udswPgEeMMbUAxpgKP7bHQ2cMKaXUAf4MBBlAodftIvs+b5OASSLyhYisEpHFvb2QiFwvIutEZF1lZeVhN2x/TQsAY3SfYqWUCvhgcRgwEVgIXAn8TUQSux9kjHncGDPXGDM3LS3tsN+0qqkdgLS4yMN+LaWUOtb5MxAUA6O9bmfa93krApYZYzqMMfnATqzA4FdVTQ7iIsOICg/191sppdRRz5+BYC0wUUSyRSQCuAJY1u2YN7B6A4hIKlaqaK8f2wRAdbODlGER/n4bpZQ6JvgtEBhjOoGfAh8A24GXjTFbReR+EVliH/YBUC0i24AVwG3GmGp/tcmtuqmdlGGaFlJKKfDj9FEAY8y7wLvd7rvb63cD/ML+OWKqmtp1xpBSStkCPVgcENVNDu0RKKWULegCgdNlqGlxkBqrYwRKKQVBGAhqWxwYg/YIlFLKFnSBoLrJAaCzhpRSyhZ0gcC9mCxVewRKKQUEdSDQHoFSSkEQBgJPaihWewRKKQXBGAia2wkNERKiwwPdFKWUOioEXyBocpAcG0GIblGplFJAEAaCqqZ2UnQNgVJKeQRhIHBo+WmllPISdIGgull7BEop5S34AoHWGVJKqS6CKhC0ODppcTh1VbFSSnkJqkDgXkOQqmsIlFLKI6gCgWdVcZz2CJRSyi2oAoGuKlZKqZ6CKxA0Wz0CHSNQSqkDgioQVGmPQCmlegiqQFDd5CA2IpToiNBAN0UppY4aQRUIqpraSdVVxUop1UVQBQJdVayUUj0FVyDQVcVKKdVDUAWCqiaH7kymlFLdBE0gcLkMNc3tOmNIKaW6CZpAUNfagcvoGgKllOouaALBgU3rtUeglFLegi4QaI9AKaW6CppA4Kk8qj0CpZTqIogCgd0j0HUESinVRdAEgvTEaM6ZNoLEGA0ESinlLSzQDThSzpk+knOmjwx0M5RS6qgTND0CpZRSvdNAoJRSQU4DgVJKBTkNBEopFeQ0ECilVJDTQKCUUkFOA4FSSgU5DQRKKRXkxBgT6DYcFBGpBPYd4tNTgapBbM6xIhjPOxjPGYLzvIPxnOHgz3usMSattweOuUBwOERknTFmbqDbcaQF43kH4zlDcJ53MJ4zDO55a2pIKaWCnAYCpZQKcsEWCB4PdAMCJBjPOxjPGYLzvIPxnGEQzzuoxgiUUkr1FGw9AqWUUt1oIFBKqSAXNIFARBaLSJ6I7BaROwLdHn8QkdEiskJEtonIVhG50b4/WUSWi8gu+8+kQLd1sIlIqIhsEJG37dvZIrLa/rxfEpEhtzWdiCSKyCsiskNEtovISUHyWd9s//veIiIviEjUUPu8ReRJEakQkS1e9/X62YrlT/a5bxKROQf7fkERCEQkFHgEOA+YBlwpItMC2yq/6ARuMcZMA+YDP7HP8w7gY2PMROBj+/ZQcyOw3ev2/wB/MMZMAGqB7wWkVf71R+B9Y8wUYBbW+Q/pz1pEMoCfA3ONMccBocAVDL3P+2lgcbf7+vpszwMm2j/XA3852DcLikAAzAN2G2P2GmMcwIvAxQFu06AzxpQaY9bbvzdiXRgysM71GfuwZ4BLAtNC/xCRTOAC4An7tgBnAK/YhwzFc04ATgP+DmCMcRhj6hjin7UtDIgWkTAgBihliH3exphPgZpud/f12V4MPGssq4BEERl1MO8XLIEgAyj0ul1k3zdkiUgWMBtYDYwwxpTaD5UBIwLULH95CPgl4LJvpwB1xphO+/ZQ/LyzgUrgKTsl9oSIxDLEP2tjTDHwALAfKwDUA18z9D9v6PuzPezrW7AEgqAiIsOA7MgH0AAAA2NJREFUV4GbjDEN3o8Za77wkJkzLCIXAhXGmK8D3ZYjLAyYA/zFGDMbaKZbGmiofdYAdl78YqxAmA7E0jOFMuQN9mcbLIGgGBjtdTvTvm/IEZFwrCDwnDHmNfvucndX0f6zIlDt84MFwBIRKcBK+Z2BlTtPtFMHMDQ/7yKgyBiz2r79ClZgGMqfNcBZQL4xptIY0wG8hvVvYKh/3tD3Z3vY17dgCQRrgYn2zIIIrMGlZQFu06Czc+N/B7YbYx70emgZ8B379+8Abx7ptvmLMeZOY0ymMSYL63P9tzHm28AK4Jv2YUPqnAGMMWVAoYhMtu86E9jGEP6sbfuB+SISY/97d5/3kP68bX19tsuAa+3ZQ/OBeq8Ukm+MMUHxA5wP7AT2AL8KdHv8dI6nYHUXNwG59s/5WDnzj4FdwEdAcqDb6qfzXwi8bf8+DlgD7Ab+BUQGun1+ON8cYJ39eb8BJAXDZw3cB+wAtgD/ACKH2ucNvIA1BtKB1fv7Xl+fLSBYsyL3AJuxZlQd1PtpiQmllApywZIaUkop1QcNBEopFeQ0ECilVJDTQKCUUkFOA4FSSgU5DQRKHUEistBdIVWpo4UGAqWUCnIaCJTqhYhcLSJrRCRXRB6z9ztoEpE/2LXwPxaRNPvYHBFZZdeCf92rTvwEEflIRDaKyHoRGW+//DCvfQSes1fIKhUwGgiU6kZEpgLfAhYYY3IAJ/BtrAJn64wx04FPgHvspzwL3G6MmYm1stN9/3PAI8aYWcDJWCtFwaoKexPW3hjjsGrlKBUwYQMfolTQORM4Hlhrf1mPxirw5QJeso/5J/CavS9AojHmE/v+Z4B/iUgckGGMeR3AGNMGYL/eGmNMkX07F8gCPvf/aSnVOw0ESvUkwDPGmDu73Cny627HHWp9lnav353o/0MVYJoaUqqnj4Fvishw8OwVOxbr/4u7wuVVwOfGmHqgVkROte+/BvjEWDvEFYnIJfZrRIpIzBE9C6V8pN9ElOrGGLNNRO4CPhSREKwKkD/B2vxlnv1YBdY4Alglgf9qX+j3Akvt+68BHhOR++3X+I8jeBpK+UyrjyrlIxFpMsYMC3Q7lBpsmhpSSqkgpz0CpZQKctojUEqpIKeBQCmlgpwGAqWUCnIaCJRSKshpIFBKqSD3/wE/s0CB4smU7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning - Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=13, activation='relu'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "          \n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = 'adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8161764815449715 using {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.7867647087311044, Stdev: 0.021792655265557744 with: {'batch_size': 10, 'epochs': 100}\n",
      "Means: 0.8161764815449715, Stdev: 0.00994365454561476 with: {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.8014706000685692, Stdev: 0.030662076028443567 with: {'batch_size': 40, 'epochs': 100}\n",
      "Means: 0.7977941198384061, Stdev: 0.030786093614443025 with: {'batch_size': 60, 'epochs': 100}\n",
      "Means: 0.8014706000685692, Stdev: 0.01713915319711339 with: {'batch_size': 80, 'epochs': 100}\n",
      "Means: 0.7904411889612675, Stdev: 0.022796230352717686 with: {'batch_size': 100, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning - Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=13, activation='relu'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "          \n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=20, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0208 10:37:56.466106 4445314496 deprecation.py:506] From /Users/JKMacBook/opt/anaconda3/envs/NN/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/adagrad.py:105: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "/Users/JKMacBook/opt/anaconda3/envs/NN/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8088235408067703 using {'optimizer': 'Nadam'}\n",
      "Means: 0.7977941198384061, Stdev: 0.03676671414234221 with: {'optimizer': 'SGD'}\n",
      "Means: 0.8051470605766072, Stdev: 0.01982087875079266 with: {'optimizer': 'RMSprop'}\n",
      "Means: 0.6066176606451764, Stdev: 0.060024674556218155 with: {'optimizer': 'Adagrad'}\n",
      "Means: 0.47426471738692594, Stdev: 0.11248907329769656 with: {'optimizer': 'Adadelta'}\n",
      "Means: 0.7977941198384061, Stdev: 0.02499154643088906 with: {'optimizer': 'Adam'}\n",
      "Means: 0.7169117815792561, Stdev: 0.035262950168783766 with: {'optimizer': 'Adamax'}\n",
      "Means: 0.8088235408067703, Stdev: 0.00620480121611568 with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning - Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=13, activation=activation))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(8, activation=activation))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = 'Nadam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=20, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8272058827912107 using {'activation': 'hard_sigmoid'}\n",
      "Means: 0.7977941198384061, Stdev: 0.03676671414234221 with: {'activation': 'softmax'}\n",
      "Means: 0.8088235408067703, Stdev: 0.02029734564007827 with: {'activation': 'softplus'}\n",
      "Means: 0.7977941296994686, Stdev: 0.013572387090618037 with: {'activation': 'softsign'}\n",
      "Means: 0.794117659330368, Stdev: 0.019763440910590686 with: {'activation': 'relu'}\n",
      "Means: 0.8088235309457078, Stdev: 0.02188304086101027 with: {'activation': 'tanh'}\n",
      "Means: 0.8235294124221101, Stdev: 0.03181525288522948 with: {'activation': 'sigmoid'}\n",
      "Means: 0.8272058827912107, Stdev: 0.033535791310007314 with: {'activation': 'hard_sigmoid'}\n",
      "Means: 0.794117659330368, Stdev: 0.026717031340633177 with: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
