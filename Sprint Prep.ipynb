{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled63.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPiNv+kW0rpQTRaKD17P/3M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JimKing100/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/Sprint%20Prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxWa-ocWOmh9",
        "colab_type": "text"
      },
      "source": [
        "### How a Neural Network and It's Components Work\n",
        "\n",
        "ANN's are computational models inspired by neural networks in the brain.  ANN's are composed of three types of layers = input, hidden and output layers.  The input layer receives inputs from the dataset and is exposed to the dataset.  The hidden layer in not exposed directly to the data and consists of weights and calculations used to analyze the data.  The output layer represents the results or output of the model.  Each layer is composed of nodes.  The complete architecture of an ANN can be visualized in a Node Map.\n",
        "\n",
        "The links between nodes in different layers represent weights.  In a feed-forward NN, each layer affects the next layer by a weighted sum of inputs plus a bias factor.  The optimal weights and biases of a NN can be searched through gradient descent if there is a loss function evaluating the quality of the predictions compared to the y values of the training data.\n",
        "\n",
        "In NN each node has an activation function.  An activation function decides how much signal to pass to the next layer.\n",
        "\n",
        "A perceptron in a simple neural network that takes input values, multiplies by weights, adds a bias,sums the products, passes the sum through an activation function and outputs a final value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbkeTBSARm7Y",
        "colab_type": "text"
      },
      "source": [
        "### Basics of Backpropagation\n",
        "\n",
        "In order to evaluate a NN's performance, data is \"fed forward\" until predictions are obtained and then the \"loss\" or \"error\" for a given observation is ascertained by looking at what the network predicted for that observation and comparing it to what it should have predicted.\n",
        "\n",
        "The error for a given observation is calculated by taking the square of the difference between the predicted value and the actual value.  The overall quality of a network's predictions can be found by finding the average error across all observations. This gives us the \"Mean Squared Error.\"\n",
        "\n",
        "An \"epoch\" is one cycle of passing our data forward through the network, measuring the error given our specified cost function, and then, via gradient descent,updating weights within our network to hopefully improve the quality of our predictions on the next iteration.\n",
        "\n",
        "Backpropagation refers to a specific algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch.\n",
        "\n",
        "4 steps for backpropagation:\n",
        "\n",
        "1) Calculate Error for a given each observation\n",
        "\n",
        "2) Does the error indicate that I'm overestimating or underestimating in my prediction?\n",
        "\n",
        "3) Look at final layer weights to get an idea for which weights are helping pass desireable signals and which are stifling desireable signals.\n",
        "\n",
        "4) Also go to the previous layer and see what can be done to boost activations that are associated with helpful weights, and limit activations that are associated with unhelpful weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUwW0HCiV07u",
        "colab_type": "text"
      },
      "source": [
        "### Build and Train a Perceptron Using Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXNge0GnOOZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbynLmyuV-w1",
        "colab_type": "text"
      },
      "source": [
        "### Build, Train and Hyperparameter Tune a MLP with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvJ7eYDIWJ9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}